
- Experiment starts should agree on a defined set of metrics to evaluate the AB test on and stick to them.
    - Additional segmenting of results can cause bias due to segments favouring personal interests or volatility, in cases of low data counts, not allowing you to evaluate the segmentation effectively.
    - If unavoidable, cutoffs for new analytics requests should be submitted to prevent crunch time for key presentations or rollout decisions.

- Experiment ends should have retros. An experiment should always being with learnings from the previous experiments and end in documenting and sharing learnings for the next experiment.

- The first experiments for a team should target metrics that are neighbouring to the location of division, e.g. to reduce chance of unaccounted or unknown external factors impacting target metric.